{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bdee8",
   "metadata": {},
   "source": [
    "# INGESTION + ETL + DB Inquinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0db5e6",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Scraper functions\n",
    "\n",
    "def _get_table_row_values(row:Tag) -> dict:\n",
    "    \"\"\"\n",
    "    Return a dict like: {\"pollutant\": value, (...)}\n",
    "    \"\"\"\n",
    "    inquinanti = [\n",
    "        \"Biossido di Zolfo\",\n",
    "        \"Polveri < 10\",\n",
    "        \"Polveri < 2.5\",\n",
    "        \"Biossido di Azoto\",\n",
    "        \"Monossido di carbonio\",\n",
    "        \"Ozono\",\n",
    "        \"Benzene\"\n",
    "    ]\n",
    "    cells = row.select(\"td\")[1:]\n",
    "    row_value = {}\n",
    "    for pollutant, cell in zip(inquinanti, cells):\n",
    "        row_value[pollutant] = cell.text\n",
    "    return row_value\n",
    "\n",
    "def get_table_day_values(day: datetime) -> dict | None:\n",
    "    \"\"\"\n",
    "    Return a dict like: {\"station_address\": {\"pollutant\": value, (...)}, (...)}\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://www.amat-mi.it/index.php?id_sezione=35&data_bollettino={day.year}-{day.month}-{day.day}\"\n",
    "    main_page = requests.get(url)\n",
    "    main_page = BeautifulSoup(main_page.text)\n",
    "    table = main_page.select_one(\".table\")\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    table_values = {}\n",
    "    stations = [\n",
    "        \"Viale Liguria\",\n",
    "        \"Viale Marche\",\n",
    "        \"Via Pascal\",\n",
    "        \"Via Senato\",\n",
    "        \"Verziere\"\n",
    "    ]\n",
    "    rows = table.select(\"tr\")[2:7]\n",
    "    for station, row in zip(stations, rows):\n",
    "        table_values[station] = _get_table_row_values(row)\n",
    "    return table_values\n",
    "\n",
    "def get_days_list(start: datetime, end: datetime):\n",
    "    numdays = (end - start).days\n",
    "    return [start + timedelta(days=x) for x in range(numdays + 1)]\n",
    "\n",
    "# Other functions\n",
    "\n",
    "def pad2(day_or_month: int) -> str:\n",
    "    response = str(day_or_month)\n",
    "    if len(response) > 1:\n",
    "        return response\n",
    "    return f\"0{response}\"\n",
    "\n",
    "days = get_days_list(datetime(2023, 1, 1), datetime(2023, 12, 31))\n",
    "db = []\n",
    "pbar = tqdm(total=len(days), desc=\"Scraping progress...\")\n",
    "for day in days:\n",
    "    table_value = get_table_day_values(day)\n",
    "    if table_value:\n",
    "        db.append({\"Date\": f\"{day.year}-{pad2(day.month)}-{pad2(day.day)}\", \"Stations\": table_value})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8536d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT = \"./Data/Raw/Inquinamento/inquinamento.json\"\n",
    "with open(PATH_OUT, \"w\") as file:\n",
    "    json.dump(db, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedea8d",
   "metadata": {},
   "source": [
    "### Normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load raw files to normalizations\n",
    "PATH_RELEVATIONS = \"./Data/Raw/Inquinamento/inquinamento.json\"\n",
    "PATH_STATIONS = \"./Data/Raw/Inquinamento/inquinamento_stazione.csv\"\n",
    "PATH_MOLECULE = \"./Data/Raw/Inquinamento/inquinamento_inquinante.csv\"\n",
    "\n",
    "with open(PATH_RELEVATIONS, 'r') as file:\n",
    "    rilevations:list = json.load(file)\n",
    "stations = pd.read_csv(PATH_STATIONS)\n",
    "inquinanti_df = pd.read_csv(PATH_MOLECULE)\n",
    "\n",
    "# Start operations\n",
    "bollettino_csv = [[\"id_bollettino\", \"data\"]]\n",
    "misurazione_csv = [[\"id_bollettino\", \"id_stazione\", \"id_inquinante\", \"valore\"]]\n",
    "for i, rilevation in enumerate(rilevations):\n",
    "    id_bollettino = i + 1\n",
    "    bollettino_csv.append([id_bollettino, rilevation[\"Date\"]])\n",
    "    for nome_stazione, inquinanti in rilevation[\"Stations\"].items():\n",
    "        id_station = stations[stations[\"nome\"] == nome_stazione][\"id_stazione\"].values[0]\n",
    "        for nome_inquinante, valore in inquinanti.items():\n",
    "            id_inquinante = inquinanti_df[inquinanti_df[\"nome\"] == nome_inquinante][\"id_inquinante\"].values[0]\n",
    "            misurazione_csv.append([id_bollettino, id_station, id_inquinante, valore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BOLLETTINO_OUT = \"./Data/Raw/Inquinamento/bollettino.csv\"\n",
    "PATH_MISURAZIONE_OUT = \"./Data/Raw/Inquinamento/misurazione.csv\"\n",
    "pd.DataFrame(bollettino_csv).to_csv(PATH_BOLLETTINO_OUT, index=False, header=False)\n",
    "pd.DataFrame(misurazione_csv).to_csv(PATH_MISURAZIONE_OUT, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386cf41",
   "metadata": {},
   "source": [
    "## ELT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ef4af",
   "metadata": {},
   "source": [
    "### Pulizia tabella misurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def lambda_valore(x):\n",
    "    valore = x.strip(\"\\n\").strip(\"\\r\").strip().replace(\",\", \".\")\n",
    "    try:\n",
    "        valore = float(valore)\n",
    "    except:\n",
    "        if \"<\" in valore:\n",
    "            valore = 0.0\n",
    "        else:\n",
    "            valore = None\n",
    "    return valore\n",
    "\n",
    "PATH_MISURAZIONE = \"./Data/Raw/Inquinamento/misurazione.csv\"\n",
    "\n",
    "misurazione = pd.read_csv(PATH_MISURAZIONE)\n",
    "\n",
    "misurazione[\"valore\"] = misurazione[\"valore\"].apply(lambda_valore)\n",
    "misurazione = misurazione[misurazione[\"valore\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MISURAZIONE_OUT = \"./Data/Clean/Inquinamento/inquinamento_misurazione_clean.csv\"\n",
    "misurazione.to_csv(PATH_MISURAZIONE_OUT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd5f57",
   "metadata": {},
   "source": [
    "### Pulizia tabelle e caricamento in Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nessuna pulizia necessaria al momento. Carico direttamente.\n",
    "\n",
    "PATH_BOLLETTINO = \"./Data/Raw/Inquinamento/bollettino.csv\"\n",
    "PATH_BOLLETTINO_OUT = \"./Data/Clean/Inquinamento/inquinamento_bollettino_clean.csv\"\n",
    "pd.read_csv(PATH_BOLLETTINO).to_csv(PATH_BOLLETTINO_OUT, index=False)\n",
    "\n",
    "PATH_STAZIONE = \"./Data/Raw/Inquinamento/inquinamento_stazione.csv\"\n",
    "PATH_STAZIONE_OUT = \"./Data/Clean/Inquinamento/inquinamento_stazione_clean.csv\"\n",
    "pd.read_csv(PATH_STAZIONE).to_csv(PATH_STAZIONE_OUT, index=False)\n",
    "\n",
    "PATH_INQUINANTE = \"./Data/Raw/Inquinamento/inquinamento_inquinante.csv\"\n",
    "PATH_INQUINANTE_OUT = \"./Data/Clean/Inquinamento/inquinamento_inquinante_clean.csv\"\n",
    "pd.read_csv(PATH_INQUINANTE).to_csv(PATH_INQUINANTE_OUT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c276e8",
   "metadata": {},
   "source": [
    "## Creation SQLITE DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_schema_table(drop_command:str, create_command:str, insert_command:str, rows :list[tuple]) -> list[str]:\n",
    "\n",
    "    for row in rows:\n",
    "        insert_command = insert_command + \"(\"\n",
    "        for element in row:\n",
    "            if isinstance(element, str):\n",
    "                element = \"'\" + element + \"'\"\n",
    "            insert_command = insert_command + f\"{element},\"\n",
    "        insert_command = insert_command.strip(\",\") + \"),\"\n",
    "    insert_command = insert_command.strip(\",\") + \";\"\n",
    "\n",
    "    return [drop_command, create_command, insert_command]\n",
    "\n",
    "def execute_query_list(queries: list[str]):\n",
    "    response = []\n",
    "    with sqlite3.connect(PATH_DB) as connection:\n",
    "        for query in queries:\n",
    "            cursor = connection.cursor()\n",
    "            result = cursor.execute(query)\n",
    "            response.append(result.fetchall())\n",
    "            connection.commit()\n",
    "    return response\n",
    "\n",
    "# Set file paths\n",
    "PATH_DB = \"./Data/Clean/Inquinamento/inquinamento.db\"\n",
    "PATH_BOLLETTINO_CLEAN = \"./Data/Clean/Inquinamento/inquinamento_bollettino_clean.csv\"\n",
    "PATH_STAZIONE_CLEAN = \"./Data/Clean/Inquinamento/inquinamento_stazione_clean.csv\"\n",
    "PATH_INQUINANTE_CLEAN = \"./Data/Clean/Inquinamento/inquinamento_inquinante_clean.csv\"\n",
    "PATH_MISURAZIONE_CLEAN = \"./Data/Clean/Inquinamento/inquinamento_misurazione_clean.csv\"\n",
    "PATH_SOGLIA_CLEAN = \"./Data/Clean/Inquinamento/inquinamento_soglia_clean.csv\"\n",
    "\n",
    "tables = {}\n",
    "# set commands tables\n",
    "tables[\"bollettino\"] = {\n",
    "    \"commands\":{\n",
    "        \"drop\": \"DROP TABLE IF EXISTS bollettino;\",\n",
    "        \"create\": \"\"\"CREATE TABLE bollettino (\"id_bollettino\" INT PRIMARY KEY,\"data\" DATE NULL);\"\"\",\n",
    "        \"insert\": \"\"\"INSERT INTO bollettino (\"id_bollettino\",\"data\") VALUES\\n\"\"\"\n",
    "    }\n",
    "}\n",
    "tables[\"stazione\"] = {\n",
    "    \"commands\": {\n",
    "        \"drop\": \"DROP TABLE IF EXISTS stazione;\",\n",
    "        \"create\": \"\"\"\n",
    "            CREATE TABLE stazione (\n",
    "                \"id_stazione\" INT PRIMARY KEY,\n",
    "                \"nome\" VARCHAR(255) NULL,\n",
    "                \"latitude\" NUMERIC NULL,\n",
    "                \"longitute\" NUMERIC NULL\n",
    "            );\n",
    "        \"\"\",\n",
    "        \"insert\":\"\"\"INSERT INTO stazione (\"id_stazione\",\"nome\",\"latitude\",\"longitute\") VALUES\\n\"\"\"\n",
    "    }\n",
    "}\n",
    "tables[\"inquinante\"] = {\n",
    "    \"commands\": {\n",
    "        \"drop\": \"DROP TABLE IF EXISTS inquinante;\",\n",
    "        \"create\": \"\"\"\n",
    "            CREATE TABLE inquinante (\n",
    "                \"id_inquinante\" INT PRIMARY KEY,\n",
    "                \"nome\" VARCHAR(255) NULL,\n",
    "                \"simbolo\" VARCHAR(255) NULL,\n",
    "                \"unita_di_misura\" VARCHAR(255) NULL,\n",
    "                \"media_temporale\" VARCHAR(255) NULL\n",
    "            );\n",
    "        \"\"\",\n",
    "        \"insert\":\"\"\"INSERT INTO inquinante (\"id_inquinante\",\"nome\",\"simbolo\",\"unita_di_misura\",\"media_temporale\") VALUES\\n\"\"\"\n",
    "    }\n",
    "}\n",
    "tables[\"misurazione\"] = {\n",
    "    \"commands\": {\n",
    "        \"drop\": \"\"\"DROP TABLE IF EXISTS misurazione;\"\"\",\n",
    "        \"create\": \"\"\"\n",
    "            CREATE TABLE misurazione (\n",
    "                \"id_bollettino\" INT NOT NULL,\n",
    "                \"id_stazione\" INT NOT NULL,\n",
    "                \"id_inquinante\" INT NOT NULL,\n",
    "                \"valore\" NUMERIC NULL,\n",
    "                FOREIGN KEY(id_bollettino) REFERENCES bollettino(id_bollettino),\n",
    "                FOREIGN KEY(id_stazione) REFERENCES stazione(id_stazione),\n",
    "                FOREIGN KEY(id_inquinante) REFERENCES inquinante(id_inquinante)\n",
    "            );\n",
    "        \"\"\",\n",
    "        \"insert\": \"\"\"INSERT INTO misurazione (\"id_bollettino\",\"id_stazione\",\"id_inquinante\",\"valore\") VALUES\\n\"\"\"\n",
    "    }\n",
    "}\n",
    "tables[\"soglia\"] = {\n",
    "    \"commands\": {\n",
    "        \"drop\": \"\"\"DROP TABLE IF EXISTS soglia;\"\"\",\n",
    "        \"create\": \"\"\"\n",
    "            CREATE TABLE soglia (\n",
    "                \"id_inquinante\" INT NOT NULL,\n",
    "                \"tipo_soglia\" VARCHAR(255) NULL,\n",
    "                \"valore\" INT NULL,\n",
    "                \"unita_di_misura\" VARCHAR(255) NULL,\n",
    "                \"periodo_di_riferimento\" VARCHAR(255) NULL,\n",
    "                \"max_superamenti_anno\" INT NULL\n",
    "            );\n",
    "        \"\"\",\n",
    "        \"insert\": \"\"\"INSERT INTO soglia (\n",
    "                        \"id_inquinante\",\n",
    "                        \"tipo_soglia\",\n",
    "                        \"valore\",\n",
    "                        \"unita_di_misura\",\n",
    "                        \"periodo_di_riferimento\",\n",
    "                        \"max_superamenti_anno\"\n",
    "                    ) VALUES\\n\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# set rows from df\n",
    "tables[\"bollettino\"][\"rows\"] = list(pd.read_csv(PATH_BOLLETTINO_CLEAN).itertuples(index=False, name=None))\n",
    "tables[\"stazione\"][\"rows\"] = list(pd.read_csv(PATH_STAZIONE_CLEAN).itertuples(index=False, name=None))\n",
    "tables[\"inquinante\"][\"rows\"] = list(pd.read_csv(PATH_INQUINANTE_CLEAN).itertuples(index=False, name=None))\n",
    "tables[\"misurazione\"][\"rows\"] = list(pd.read_csv(PATH_MISURAZIONE_CLEAN).itertuples(index=False, name=None))\n",
    "tables[\"soglia\"][\"rows\"] = list(pd.read_csv(PATH_SOGLIA_CLEAN).itertuples(index=False, name=None))\n",
    "\n",
    "# Create schema query\n",
    "total_query = []\n",
    "for v in tables.values():\n",
    "    total_query.extend(get_schema_table(v[\"commands\"][\"drop\"], v[\"commands\"][\"create\"], v[\"commands\"][\"insert\"], v[\"rows\"]))\n",
    "\n",
    "# Execute query\n",
    "query_result = execute_query_list(total_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef12bf",
   "metadata": {},
   "source": [
    "# Nuovo Inquinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69a02b",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec67ee",
   "metadata": {},
   "source": [
    "Otteniamo gli id delle stazioni di Milano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6f47ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://airnet.waqi.info/airnet/validator/check/112273\"\n",
    "risposta = requests.get(URL)\n",
    "risposta_json = risposta.json()\n",
    "neighbors = risposta_json[\"data\"][\"neighbors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c909b97",
   "metadata": {},
   "source": [
    "inizializziamo il geojson con id e coordinate delle stazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cac211",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"name\": \"Inquinamento\",\n",
    "    \"crs\": {\n",
    "        \"type\": \"name\",\n",
    "        \"properties\": {\n",
    "            \"name\": \"urn:ogc:def:crs:EPSG::6707\"\n",
    "        }\n",
    "    },\n",
    "    \"features\": []\n",
    "}\n",
    "for geo, id in zip(neighbors[\"geos\"], neighbors[\"ids\"]):\n",
    "    if not id == 353767: # Stazione marco emme, Milan, Italy che non contiene dati pm10 e pm25\n",
    "        geojson[\"features\"].append(\n",
    "            {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\n",
    "                    \"id\": id,\n",
    "                    \"longitudine\": geo[1],\n",
    "                    \"latitudine\": geo[0],\n",
    "                    \"name\": \"\",\n",
    "                    \"pm10\": [],\n",
    "                    \"pm25\": []\n",
    "                },\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [geo[1], geo[0]]\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3b0e2",
   "metadata": {},
   "source": [
    "Estraiamo i dati di ogni stazione (per i 2 inquinanti) e li inseriamo direttamente tra le \"features\" >> \"properties\" del geojson inizializzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2598976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping progress...:   6%|▌         | 22/365 [00:32<08:28,  1.48s/it]\n",
      "Scraping progress...: 100%|██████████| 19/19 [00:57<00:00,  3.22s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from sseclient import SSEClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(total=len(geojson[\"features\"]), desc=\"Scraping progress...\")\n",
    "\n",
    "for feature in geojson[\"features\"]:\n",
    "    for inquinante in [\"pm10\", \"pm25\"]:\n",
    "        url = f\"https://airnet.waqi.info/airnet/sse/historic/daily/{feature[\"properties\"][\"id\"]}?specie={inquinante}\"\n",
    "        response = requests.get(url, params={\"specie\": inquinante}, headers={\"Accept\": \"text/event-stream\"}, stream=True)\n",
    "        client = SSEClient(response)\n",
    "        for i, event in enumerate(client.events()):\n",
    "            if event.event != \"error\":\n",
    "                try:\n",
    "                    data = json.loads(event.data)\n",
    "                    if i == 0:\n",
    "                        feature[\"properties\"][\"name\"] = data[\"meta\"][\"name\"]\n",
    "                    elif isinstance(data, dict): #Ogni tanto ci sono righe sporche di stringhe\n",
    "                        feature[\"properties\"][inquinante].append(data)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Errore JSON:\", event.data)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c8257",
   "metadata": {},
   "source": [
    "salvo il file geojson creato in Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INQUINAMENTO_GEOJSON_STAGING = \"./Data/Raw/Inquinamento/inquinamento_staging.geojson\"\n",
    "with open(PATH_INQUINAMENTO_GEOJSON_STAGING, 'w') as file:\n",
    "    json.dump(geojson, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3db842",
   "metadata": {},
   "source": [
    "## Elaborazione dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e11d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PATH_INQUINAMENTO_GEOJSON_STAGING = \"./Data/Raw/Inquinamento/inquinamento_staging.geojson\"\n",
    "with open(PATH_INQUINAMENTO_GEOJSON_STAGING, 'r') as file:\n",
    "    geojson = json.load(file)\n",
    "features = geojson[\"features\"]\n",
    "\n",
    "for feature in geojson[\"features\"]:\n",
    "    for data in [\"pm10\", \"pm25\"]:\n",
    "        df = pd.DataFrame(feature[\"properties\"][data])\n",
    "        df['day'] = pd.to_datetime(df['day'])\n",
    "        filtered_df = df[df['day'].dt.year.isin([2023])].reset_index()\n",
    "        if len(filtered_df) > 200:\n",
    "            media_median = filtered_df[\"median\"].mean()\n",
    "            feature[\"properties\"][data] = media_median\n",
    "        else:\n",
    "            feature[\"properties\"][data] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e129f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INQUINAMENTO_GEOJSON_CLEAN = \"./Data/Clean/Inquinamento/inquinamento.geojson\"\n",
    "with open(PATH_INQUINAMENTO_GEOJSON_CLEAN, 'w') as file:\n",
    "    json.dump(geojson, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
